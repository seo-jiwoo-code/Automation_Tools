{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #doing simple getting website \n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd \n",
    "\n",
    "# search_url_prefix = \"https://www.google.com/search?q=\"\n",
    "\n",
    "# df = pd.read_excel(\"Shopping_Malls.xlsx\")\n",
    "\n",
    "# all_info = []\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     mall_name = row[\"Name\"]\n",
    "    \n",
    "#     search_url = search_url_prefix + mall_name\n",
    "#     r = requests.get(search_url)\n",
    "#     soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "#     soup.append(info)\n",
    "        \n",
    "# # Save the information to a new Excel file\n",
    "# save_to_excel(all_info, \"all_info.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_first_result(search_str):\n",
    "#     search_url = search_url_prefix + search_str\n",
    "#     r = requests.get(search_url)\n",
    "#     soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "#     return soup.find('cite').text\n",
    "\n",
    "# companies = ['Google', 'Microsoft']\n",
    "# for company in companies:\n",
    "#     result = get_first_result(company)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the list of shopping malls from an Excel file\n",
    "# df = pd.read_excel(\"Shopping_Malls.xlsx\")\n",
    "\n",
    "\n",
    "# # Initialize an empty list to store the information\n",
    "# all_info = []\n",
    "\n",
    "# # Loop over each row in the Excel file\n",
    "# for index, row in df.iterrows():\n",
    "#     mall_name = row[\"Name\"]\n",
    "    \n",
    "#     # Search for the website of the shopping mall\n",
    "#     website = search_website(mall_name)\n",
    "    \n",
    "#     # If a website was found, extract the information\n",
    "#     if website:\n",
    "#         info = extract_info(website)\n",
    "#         all_info.append(info)\n",
    "        \n",
    "# # Save the information to a new Excel file\n",
    "# save_to_excel(all_info, \"all_info.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# import urllib.parse\n",
    "\n",
    "# file_path = '/Users/seojiwoo/Desktop/Shopping_Malls.xlsx'\n",
    "# df = pd.read_excel(file_path)\n",
    "\n",
    "# all_queries = df['Name'].tolist()\n",
    "\n",
    "# search_url_prefix = \"https://www.google.com/search?q=\"\n",
    "\n",
    "# def get_first_result(search_str):\n",
    "#     search_url_with_space = search_url_prefix + search_str\n",
    "#     search_url=search_url_with_space.replace(\" \", \"%\")\n",
    "#     r = requests.get(search_url)\n",
    "#     soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "#     result = soup.find('cite')\n",
    "#     if result:\n",
    "#         return result.text\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# for query in all_queries:\n",
    "#     result = get_first_result(query)\n",
    "#     print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!pip install simplejson\n",
    "\n",
    "# import simplejson\n",
    "\n",
    "# def search(query):\n",
    "#     api_key = 'AIzaSyCisrQsOpJPpqLHpnM8FmJzM47kyYnZV5Y'\n",
    "#     search_engine_id = 'f7180b0c658f4467a'\n",
    "#     url = \"https://www.googleapis.com/customsearch/v1/siterestrict?key=%s&cx=%s&q=%s\" % (api_key, search_engine_id, query)\n",
    "#     result = requests.Session().get(url)\n",
    "#     json = simplejson.loads(result.content)\n",
    "#     return json\n",
    "\n",
    "# file_path = '/Users/seojiwoo/Desktop/Shopping_Malls.xlsx'\n",
    "# df = pd.read_excel(file_path)\n",
    "\n",
    "# all_queries = df['Name'].tolist()\n",
    "\n",
    "# for query in all_queries:\n",
    "#     result = search(query)\n",
    "#     print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff = pd.DataFrame(result)\n",
    "# file_path = '/Users/seojiwoo/Desktop/Shopping_Malls.xlsx'\n",
    "# dff.to_excel(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd \n",
    "\n",
    "# file_path = '/Users/seojiwoo/Desktop/Shopping_Malls.xlsx'\n",
    "# df = pd.read_excel(file_path)\n",
    "\n",
    "# all_info = df['Name'].tolist()\n",
    "\n",
    "\n",
    "# search_url_prefix = \"https://www.google.com/search?q=\"\n",
    "\n",
    "# def get_first_result(search_str):\n",
    "#     search_url = search_url_prefix + search_str\n",
    "#     r = requests.get(search_url)\n",
    "#     soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "#     cite = soup.find('cite')\n",
    "#     if cite is not None:\n",
    "#         return cite.text\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# for shop in all_info:\n",
    "#     result = get_first_result(shop)\n",
    "#     print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets all of the URLs and append to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this performs nicely. just need to clean it up afterwards \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd \n",
    "\n",
    "file_path = '/Users/seojiwoo/Desktop/Shopping_Malls.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "all_info = df['Name'].tolist()\n",
    "\n",
    "url = 'https://www.google.com/search'\n",
    "\n",
    "headers = {\n",
    "\t'Accept' : '*/*',\n",
    "\t'Accept-Language': 'en-US,en;q=0.5',\n",
    "\t'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "}\n",
    "\n",
    "def get_first_result(search):\n",
    "    parameters = {'q': search}\n",
    "    content = requests.get(url, headers = headers, params = parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    search = soup.find(id = 'search')\n",
    "    first_link = search.find('a')\n",
    "    return (first_link['href'])\n",
    "\n",
    "for shop in all_info:\n",
    "    result = get_first_result(shop)\n",
    "    print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)\n",
    "\n",
    "#this is string, i just copy and pasted afterwards on the xlsx file LOL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # read the excel file into a pandas dataframe\n",
    "# file_path = '/Users/seojiwoo/Desktop/Shopping_Malls.xlsx'\n",
    "# df = pd.read_excel(file_path)\n",
    "\n",
    "# # loop through each row in the dataframe\n",
    "# for index, row in df.iterrows():\n",
    "#     url = row[\"URL\"]\n",
    "    \n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "#     # extract all text from the page\n",
    "#     text = soup.get_text()\n",
    "\n",
    "#     # use regex to search for email addresses\n",
    "#     emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\", text)\n",
    "\n",
    "#     # use regex to search for phone numbers\n",
    "#     phones = re.findall(r\"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\", text)\n",
    "    \n",
    "#     # store the results back in the dataframe\n",
    "#     df.loc[index, \"Emails\"] = \",\".join(emails)\n",
    "#     df.loc[index, \"Phone Numbers\"] = \",\".join(phones)\n",
    "\n",
    "# # save the updated dataframe back to the excel file\n",
    "# df.to_excel(\"websites_contacts.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same code as above, but with the timeout parameter \n",
    "\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# read the excel file into a pandas dataframe\n",
    "file_path = '/Users/seojiwoo/Desktop/Shopping_Malls.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# loop through each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    url = row[\"URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # extract all text from the page\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # use regex to search for email addresses\n",
    "        emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\", text)\n",
    "\n",
    "        # use regex to search for phone numbers\n",
    "        phones = re.findall(r\"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\", text)\n",
    "\n",
    "        # store the results back in the dataframe\n",
    "        df.loc[index, \"Emails\"] = \",\".join(emails)\n",
    "        df.loc[index, \"Phone Numbers\"] = \",\".join(phones)\n",
    "    except:\n",
    "        # print an error message and continue to the next URL\n",
    "        print(\"Error accessing\", url)\n",
    "        continue\n",
    "\n",
    "# save the updated dataframe back to the excel file\n",
    "# df.to_excel(\"websites_contacts.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/seojiwoo/Desktop/result.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this performs nicely. just need to clean it up afterwards \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd \n",
    "\n",
    "file_path = '/Users/seojiwoo/Desktop/Shopping_Malls.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "all_info = df['Name'].tolist()\n",
    "\n",
    "url = 'https://www.google.com/search'\n",
    "\n",
    "headers = {\n",
    "\t'Accept' : '*/*',\n",
    "\t'Accept-Language': 'en-US,en;q=0.5',\n",
    "\t'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "}\n",
    "\n",
    "def get_first_result(search):\n",
    "    parameters = {'q': search}\n",
    "    content = requests.get(url, headers = headers, params = parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    search = soup.find(id = 'search')\n",
    "    first_link = search.find('a')\n",
    "    return (first_link['href'])\n",
    "\n",
    "for shop in all_info:\n",
    "    result = get_first_result(shop)\n",
    "    print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing https://www.capitaland.com/sg/malls/aperia/en.html\n",
      "Error accessing https://www.hdb.gov.sg/residential/where2shop/explore/kallang-whampoa/balestier-hill-shopping-centre\n",
      "Error accessing https://www.capitaland.com/sg/malls/bugisjunction/en.html\n",
      "Error accessing https://www.capitaland.com/sg/malls/bugisplus/en.html\n"
     ]
    }
   ],
   "source": [
    "#this will follow the 'contact us' \n",
    "\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "\t'Accept' : '*/*',\n",
    "\t'Accept-Language': 'en-US,en;q=0.5',\n",
    "\t'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93',\n",
    "} \n",
    "\n",
    "# read the excel file into a pandas dataframe\n",
    "file_path = '/Users/seojiwoo/Desktop/Shopping_Malls_Test.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url = row[\"URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers = headers, timeout=40)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # search for a link with the text \"Contact Us\"\n",
    "        contact_us_link = soup.find(\"a\", text=\"Contact Us\")\n",
    "        if contact_us_link:\n",
    "            # if the link exists, follow it\n",
    "            response = requests.get(contact_us_link[\"href\"], headers= headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # extract all text from the page\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # use regex to search for email addresses\n",
    "        emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\", text)\n",
    "\n",
    "        # use regex to search for phone numbers\n",
    "        phones = re.findall(r\"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\", text)\n",
    "\n",
    "        # store the results back in the dataframe\n",
    "        df.loc[index, \"Emails\"] = \",\".join(emails)\n",
    "        df.loc[index, \"Phone Numbers\"] = \",\".join(phones)\n",
    "    except:\n",
    "        # print an error message and continue to the next URL\n",
    "        print(\"Error accessing\", url)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting links from https://www.100am.com.sg/\n",
      "Error getting links from https://www.313somerset.com.sg/\n",
      "Error getting links from https://www.capitaland.com/sg/malls/aperia/en.html\n",
      "Error getting links from https://www.hdb.gov.sg/residential/where2shop/explore/kallang-whampoa/balestier-hill-shopping-centre\n",
      "Error getting links from https://www.facebook.com/BugisCubeMall/\n",
      "Error getting links from https://www.capitaland.com/sg/malls/bugisjunction/en.html\n",
      "Error getting links from https://www.capitaland.com/sg/malls/bugisplus/en.html\n",
      "Error getting links from https://capitolsingapore.com/\n"
     ]
    }
   ],
   "source": [
    "# this is an improved version \n",
    "\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "\t'Accept' : '*/*',\n",
    "\t'Accept-Language': 'en-US,en;q=0.5',\n",
    "\t'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "} \n",
    "\n",
    "\n",
    "def get_all_links(url):\n",
    "    links = []\n",
    "    try:\n",
    "        response = requests.get(url, headers = headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for link in soup.find(\"a\",text=\"Contact us\"):\n",
    "            link_url = link.get(\"href\")\n",
    "            if link_url is not None and link_url.startswith(\"http\"):\n",
    "                links.append(link_url)\n",
    "    except:\n",
    "        print(\"Error getting links from\", url)\n",
    "    return links\n",
    "    \n",
    "# read the excel file into a pandas dataframe\n",
    "file_path = '/Users/seojiwoo/Desktop/Shopping_Malls_Test.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# create a set to keep track of visited links\n",
    "visited_links = set()\n",
    "\n",
    "# loop through each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    url = row[\"URL\"]\n",
    "\n",
    "    try:\n",
    "        # get all the links on the page\n",
    "        links = get_all_links(url)\n",
    "\n",
    "        for link in links:\n",
    "            if link not in visited_links:\n",
    "                visited_links.add(link)\n",
    "                response = requests.get(link, timeout=10)\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # extract all text from the page\n",
    "                text = soup.get_text()\n",
    "\n",
    "                # use regex to search for email addresses\n",
    "                emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\", text)\n",
    "\n",
    "                # use regex to search for phone numbers\n",
    "                phones = re.findall(r\"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\", text)\n",
    "\n",
    "                # store the results back in the dataframe\n",
    "                df.loc[index, \"Emails\"] = \",\".join(emails)\n",
    "                df.loc[index, \"Phone Numbers\"] = \",\".join(phones)\n",
    "    except Exception as e:\n",
    "        print(\"Error getting links from\", url)\n",
    "        print(\"Exception:\", e)\n",
    "        continue\n",
    "\n",
    "\n",
    "df.to_csv('/Users/seojiwoo/Desktop/result2.csv', index=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(visited_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code that searches the site \"Contact us\" then finds the email and contact as requested. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec28c98a28ba2ed18a38fcecdfd860e29d2fa4b20ca69680a4795f251f325b67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
